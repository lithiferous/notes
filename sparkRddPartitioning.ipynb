{
  "metadata": {
    "name": "spark_rdd_partitioning",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "* costly communication\n* laying out data to minimize network can greatly improve performance\n* useful only in cross setting such as joins\n* especially when rdd collect is made multiple times\n* allows organising partitioning either by modulo or --\u003e by ordered keys"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "*example user program on uid/uinfo_topics table with updated events each 5 minutes that counts up a topic with unsubscribed tag*"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n//do not provide info to Spark which partition for uid is located\n\nval sc \u003d new SparkContext(...)\nval userData \u003d sc.sequenceFile[UserID, UserInfo](\"hdfs://...\").persist()\n\n// SequenceFile caontains (uid/ LinkInfo) pairs\n\ndef processNewLogs(logFileName: String) {\n    val events \u003d sc.SequenceFile[UserId, LinkInfo](logFileName)\n    val joined \u003d userData.join(events) // (uid, (uinfo, linkinfo))\n    val offTopicVisits \u003d joined.filter{\n        case (userId, (userInfo, linkInfo) \u003d\u003e (\n            !userInfo.topics.contains(linkInfo.topic)\n        )\n    }.count()\n    println(\"Number of visits to non-subscribed topics: \" + offTopicVisits)\n}"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "*problem of the program above is hashing then joining and reshuffling userData table each time the call is made, however it is fixed*"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n//quickfix\nval sc \u003d new SparkContext(...)\nval userData \u003d sc.sequenceFile[UserID, UserInfo](\"hdfs://...\")\n                 .partitionBy(new HashPartitioner(100)) //as large as the number of cores on my cluster\n                 .persist()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Operations that benefit from partitioning\n* cogroup //likeOuterJoin returns iterableinterface\n* groupWith //\n* join\n* leftOuterJoin // Леша без пересечений\n* rightOuterJoin // Таня без пересечений\n* groupByKey //valueCounts\n* reduceByKey //map reducing func on key eg x+y\n* combineByKey //\n* lookup\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Operations that are affected by partitioningУ\n**Result from those operations does not imply producing RDD with a partitioner**\n* cogroup //likeOuterJoin returns iterableinterface\n* groupWith //\n* join\n* leftOuterJoin // Леша без пересечений\n* rightOuterJoin // Таня без пересечений\n* groupByKey //valueCounts\n* combineByKey //\n* partitionBy\n* sort\n* mapValues // user whenever key stays same as it \n* flatMapValues // preserves hashcodes\n* filter\n**reduceByKey is hash partitioned!**"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Custom Partitioners\n*While HashPartitioner and RangePartitioner are well suited*\nAs an example imagine a set of links that can be hashed there is a significant overlay between cnn.com/world and cnn.com/US, so we could insist that HashPartitioner is used effectively by specifying domain name scope "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Implementing a custom partitioner**\n* *numPartitions (Int)*\n* *getPartitione(key: Any) (Int)*\n* *equals*"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nimport java.net.URL\nimport org.apache.spark.Partitioner\n\nclass DomainNamePartitioner(numParts: Int) extends Partitioner {\n    override def numPartitions: Int \u003d numParts\n    override def getPartition(key: Any): Int \u003d {\n        val domain \u003d new URL(key.toString).getHost()\n        val code \u003d (domain.hashCode % numPartitions)\n        if (code \u003c 0) code + numPartitions\n        else code\n    }\n\n    override def equals(other: Any): Boolean \u003d other match {\n        case dnp: DomainNamePartitioner \u003d\u003e\n            dnp.numPartitions \u003d\u003d numPartitions\n        case _ \u003d\u003e\n            false\n    }\n}\n// Use - join, groupByKey, PartitionBy"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    }
  ]
}