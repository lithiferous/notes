{
  "metadata": {
    "name": "spark_rdds",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n### Transformations\n\ndef: *Operations on RDD that return a new RDD, among them map/filter*"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval in \u003d sc.textFile(\"/home/bane/projects/data/log.txt\")\nval err \u003d in.filter(l \u003d\u003e \"err\" contains l)\nval war \u003d in.filter(l \u003d\u003e \"warning\" contains l)\nval bads \u003d err.union(war)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " #### **Map**"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval in \u003d sc.parallelize(List(1,2,3,4))\nval res \u003d in.map(x \u003d\u003e x * x)\nprintln(s\"Result is: ${res.collect().mkString(\",\")}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " #### **FlatMap**"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval in \u003d sc.parallelize(List(\"hello world\", \"hi\"))\nval words \u003d in.flatMap(l \u003d\u003e l.split(\" \"))\nwords.first"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " #### **Pseudo set ops**"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### *Distinct + shuffle*"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "rdd.distinct"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### *Union - shuffle*"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "rdd.union(rdd2)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### *Intersection + shuffle*"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "rdd.intersection(rdd2)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### *Subtract + shuffle*"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "rdd.subtract(rdd2)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Actions\ndef: *Operations returning the result to driver and performing computation aka count/first*"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nprintln(s\"Corrupt input ${bads.count} concerning lines\")\nprintln(s\"Here are 2 examples:\")\nbads.take(2).foreach(println)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### *Reduce*"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \nval sum \u003d rdd.reduce((x, y) \u003d\u003e x + y)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### *Aggregate*"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "val res \u003d in.aggregate((0, 0),\n                       (acc, value) \u003d\u003e (acc._1 + value, acc._2 + 1),\n                       (acc1, acc2) \u003d\u003e (acc1._1 + acc2._1, acc1._2 + acc2._2))\nval avg \u003d res._1 / res._2.toDouble"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### *Collect*\n* returns entire RDD\n* has to fit in mem of one machine"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### *Take(n)*\n* returns n elems RDD\n* attempts to minimize num of partitions it accesses\n\u003d\u003e elems not in sequential order"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### *top*\n* returns n elems RDD\n* elems in sequential order"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### *takeSample(withReplacement, num, seed)*\n* random data sampling"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### *foreach*\n* does not return values locally\n* performs computation say inserting records to Db or by posting JSON"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### *countByValue*\n* returns a map of each unique val to its count"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Passing functions to Spark"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nimport org.apache.spark.rdd.RDD\n\nclass SearchFunction(val query: String){\n    def isMatch(s: String): Boolean \u003d {\n        s.contains(query)\n    }\n    def getMatchesFunctionReference(rdd: RDD[String]): RDD[String] \u003d {\n        rdd.map(isMatch)\n    }\n    def getMatcheesFieldReference(rdd: RDD[String]): RDD[String] \u003d {\n        rdd.map(x \u003d\u003e x.split(query))\n    }\n    def getMatchesNotReference(rdd: RDD[String]): RDD[String] \u003d {\n        val query_ \u003d this.query\n        rdd.map(x \u003d\u003e x.split(query_))\n    }\n}"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Converting between RDD types"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "*Mean, Variance*"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nimport org.apache.spark.SparkContext._"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Persistence (Caching)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### *Avoid recomputing*\nval res \u003d in.map(x \u003d\u003e x * x)\nprintln(res.count)\nprintln(res.collect.mkString(\",\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "* nodes store data of partitions\n* recompute lost data on nodes\n* allows replicating partitions to avoid slowdown"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "import org.apache.spark.storage.StorageLevel\nval result in.map(x \u003d\u003e x * x)\nres.persist(StorageLevel.DISK_ONLY)\nprintln(res.count)\nprintln(res.collect.mkString(\",\"))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### *Persistance levels*\n* Level/Space/Cpu time/In mem/On disk/Comments\n* MEMORY_ONLY/High/Low/Y/N\n* MEMORY_ONLY_SER/Low/High/Y/N\n* MEMORY_AND_DISK/High/Med/Some/Some/Splits to disk on MEM overflow\n* MEMORY_AND_DISK_SER/Low/High/Some/Some/Stores serialized repr in MEM\n* DISK_ONLY/Low/High/N/Y"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ""
    }
  ]
}